{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46846350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.ops import mean , max\n",
    "from keras.layers import Concatenate , Reshape , Conv2D , BatchNormalization , Activation , Multiply , Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed240bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow is channel last\n",
    "class resnetTripletAtt() : \n",
    "    def __init__(self , k_size):\n",
    "        self.k_size = k_size\n",
    "    #---\n",
    "    def __Z_pool(self,tensor) : \n",
    "        avgpool = mean(tensor , axis=-1 , keepdims=True)\n",
    "        maxpool = max(tensor  , axis=-1 , keepdims=True)\n",
    "        concat  = Concatenate(axis=-1)([maxpool , avgpool])\n",
    "        return concat\n",
    "    #---\n",
    "    def __branch_H_C(self , tensor): # branch one\n",
    "        channel = tensor.shape(-1)\n",
    "        width   = tensor.shape(-2)\n",
    "        height  = tensor.shape(-3) \n",
    "        tensor_hat             = Reshape((height , channel , width))(tensor)\n",
    "        tensor_hat_star        = self.__Z_pool(tensor_hat)\n",
    "        tensor_hat_star_conv   = Conv2D(filters=1 , kernel_size=self.k_size , strides=1 , padding=\"same\")(tensor_hat_star)\n",
    "        tensor_hat_star_conv_N = BatchNormalization(axis=-1)(tensor_hat_star_conv)\n",
    "        attention_map          = Activation(activation='sigmoid')(tensor_hat_star_conv_N)\n",
    "        attention_out  = Multiply()([tensor_hat , attention_map])\n",
    "        rotated_tensor = Reshape((height , width , channel))(attention_out)\n",
    "        return rotated_tensor\n",
    "    def __branch_W_C(self , tensor): # branch two\n",
    "        channel = tensor.shape(-1)\n",
    "        width   = tensor.shape(-2)\n",
    "        height  = tensor.shape(-3) \n",
    "        tensor_hat             = Reshape((channel , width , height))(tensor)\n",
    "        tensor_hat_star        = self.__Z_pool(tensor_hat)\n",
    "        tensor_hat_star_conv   = Conv2D(filters=1 , kernel_size=self.k_size , strides=1 , padding=\"same\")(tensor_hat_star)\n",
    "        tensor_hat_star_conv_N = BatchNormalization(axis=-1)(tensor_hat_star_conv)\n",
    "        attention_map          = Activation(activation='sigmoid')(tensor_hat_star_conv_N)\n",
    "        attention_out  = Multiply()([tensor_hat , attention_map])\n",
    "        rotated_tensor = Reshape((height , width , channel))(attention_out)\n",
    "        return rotated_tensor\n",
    "    def __branch_identify(self , tensor): # branch identify (third branch)\n",
    "        tensor_hat        = self.__Z_pool(tensor)\n",
    "        tensor_hat_conv   = Conv2D(filters=1 , kernel_size=self.k_size , strides=1 , padding=\"same\")(tensor_hat)\n",
    "        tensor_hat_conv_N = BatchNormalization(axis=-1)(tensor_hat_conv)\n",
    "        attention_map          = Activation(activation='sigmoid')(tensor_hat_conv_N)\n",
    "        attention_out  = Multiply()([tensor_hat , attention_map])\n",
    "        return attention_out\n",
    "    def triplet_Attention(self , tensor) :\n",
    "        br_1_out  = self.__branch_H_C(tensor)\n",
    "        br_2_out  = self.__branch_W_C(tensor)\n",
    "        br_3_out  = self.__branch_identify(tensor)\n",
    "        #---\n",
    "        tensor_out = Multiply()([Add()([br_1_out , br_2_out , br_3_out])] , 1/3)\n",
    "    def __call__(self , tensor):\n",
    "        return self.triplet_Attention(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
